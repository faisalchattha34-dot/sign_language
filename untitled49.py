# -*- coding: utf-8 -*-
"""Untitled49.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rIMe4BXRjaFFAt36K7QJUv8uQNYjUqNq
"""

from google.colab import drive
drive.mount('/content/drive/')
# Recommended path for saving:
# /content/drive/MyDrive/yolov8_asl_runs

# install ultralytics (includes yolo CLI and python API)
!pip install -U ultralytics
# Optional: for visualization
!pip install -U roboflow seaborn matplotlib

!ls /content/drive/MyDrive/

!cp -r "/content/drive/MyDrive/ASL.v1-v1.yolov8" /content/ASL_dataset

import yaml, os

# Path to your data.yaml file (update if needed)
yaml_path = "/content/drive/MyDrive/ASL.v1-v1.yolov8/data.yaml"

# âœ… Step 1 â€” Overwrite data.yaml with correct absolute paths
with open(yaml_path, "w") as f:
    f.write("""train: /content/drive/MyDrive/ASL.v1-v1.yolov8/train/images
val: /content/drive/MyDrive/ASL.v1-v1.yolov8/valid/images
test: /content/drive/MyDrive/ASL.v1-v1.yolov8/test/images

nc: 26
names: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]
""")

# âœ… Step 2 â€” Read & print updated YAML
with open(yaml_path) as f:
    cfg = yaml.safe_load(f)

print("âœ… Updated data.yaml contents:\n")
print(yaml.dump(cfg, sort_keys=False))

# âœ… Step 3 â€” Verify if image folders exist
for split in ['train', 'val', 'test']:
    dir_path = cfg.get(split)
    if os.path.exists(dir_path) and os.path.isdir(dir_path):
        print(f"\nâœ… {split} folder found: {dir_path}")
        print("Sample files:", os.listdir(dir_path)[:3])
    else:
        print(f"\nâŒ Directory not found or is not a directory: {dir_path}")

import os

base = "/content/drive/MyDrive/ASL.v1-v1.yolov8"

for split in ['train', 'valid', 'test']:
    img_dir = os.path.join(base, split, "images")
    lbl_dir = os.path.join(base, split, "labels")

    imgs = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
    lbls = [f for f in os.listdir(lbl_dir) if f.endswith('.txt')]

    print(f"\nðŸ“‚ {split.upper()}: {len(imgs)} images, {len(lbls)} labels")
    print("Sample images:", imgs[:3])
    print("Sample labels:", lbls[:3])

# Make a copy of data.yaml with absolute paths (safer)
import yaml, os
# Correct path to the original data.yaml file in Google Drive
yaml_path_drive = "/content/drive/MyDrive/ASL.v1-v1.yolov8/data.yaml"
base = "/content/ASL_dataset" # This is where the dataset was intended to be copied

# Ensure the base directory exists
os.makedirs(base, exist_ok=True)

with open(yaml_path_drive) as f:
    cfg = yaml.safe_load(f)

# If train/val are relative, convert to absolute paths based on the dataset location in /content
for k in ('train','val','test'):
    if k in cfg:
        p = cfg[k]
        # Assuming the relative paths in data.yaml are relative to the dataset's root directory
        # which was intended to be copied to /content/ASL_dataset
        cfg[k] = os.path.join(base, p)


# write new file in the /content/ASL_dataset directory
new_yaml = os.path.join(base, "data_abs.yaml")
with open(new_yaml, "w") as f:
    yaml.safe_dump(cfg, f)

print("Wrote", new_yaml)
print(yaml.dump(cfg, sort_keys=False))

# Using CLI (simple). Change epochs, imgsz as needed.
# Save runs to Drive so you won't lose them when the session disconnects:
OUTPUT_DIR="/content/drive/MyDrive/yolov8_asl_runs"
!mkdir -p $OUTPUT_DIR

# Example: train yolov8n pretrained as backbone, output saved to runs (and copy to Drive after)
!yolo train model=yolov8n.pt data=/content/ASL_dataset/data_abs.yaml epochs=50 imgsz=640 project=$OUTPUT_DIR name=exp1

# Evaluate the trained model on the test set
# Replace 'path/to/your/best.pt' with the actual path to your trained model weights
# The weights are usually saved in the runs/detect/train/weights directory within your output directory
OUTPUT_DIR="/content/drive/MyDrive/yolov8_asl_runs"
# You'll need to find the exact path to the best weights file, it's usually in a directory like:
# /content/drive/MyDrive/yolov8_asl_runs/exp/weights/best.pt
# Based on the previous output, the run directory seems to be exp13, so the path might be:
# /content/drive/MyDrive/yolov8_asl_runs/exp13/weights/best.pt
# Please verify the exact path in your Google Drive
MODEL_PATH = f"{OUTPUT_DIR}/exp13/weights/best.pt"

!yolo detect val model={MODEL_PATH} data=/content/ASL_dataset/data_abs.yaml

!ls -lh /content/drive/MyDrive/yolov8_asl_runs/exp13/weights

from ultralytics import YOLO

model = YOLO("/content/drive/MyDrive/yolov8_asl_runs/exp13/weights/best.pt")

!yolo detect predict model="/content/drive/MyDrive/yolov8_asl_runs/exp13/weights/best.pt" source="/content/drive/MyDrive/ASL.v1-v1.yolov8/test/images"

!mkdir -p /content/asl_streamlit/models
!cp "/content/drive/MyDrive/yolov8_asl_runs/exp13/weights/best.pt" /content/asl_streamlit/models/

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/asl_streamlit/requirements.txt
# streamlit>=1.20
# ultralytics>=8.3.0
# opencv-python-headless
# pillow
# numpy
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/asl_streamlit/app.py
# import streamlit as st
# from ultralytics import YOLO
# import cv2
# import numpy as np
# from PIL import Image
# import tempfile
# import os
# from collections import deque, Counter
# 
# st.set_page_config(layout="wide", page_title="ASL Detection - YOLOv8")
# 
# st.title("ðŸ¤– American Sign Language Detection (YOLOv8)")
# 
# MODEL_PATH = "models/best.pt"
# 
# @st.cache_resource
# def load_model():
#     model = YOLO(MODEL_PATH)
#     return model
# 
# model = load_model()
# 
# st.sidebar.header("Settings")
# mode = st.sidebar.radio("Choose mode:", ["Image Upload", "Use Camera", "Video Upload"])
# conf = st.sidebar.slider("Confidence threshold", 0.1, 0.9, 0.25)
# imgsz = st.sidebar.selectbox("Image size", [320, 480, 640, 896], index=2)
# 
# def run_inference(img):
#     res = model.predict(source=img, conf=conf, imgsz=imgsz)
#     out = res[0].plot()
#     boxes = res[0].boxes
#     labels = []
#     if boxes is not None and len(boxes) > 0:
#         for box in boxes:
#             cls = int(box.cls[0])
#             conf_score = float(box.conf[0])
#             labels.append((model.names[cls], conf_score))
#     return out[:, :, ::-1], labels
# 
# # ----- Image upload -----
# if mode == "Image Upload":
#     file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
#     if file:
#         img = Image.open(file).convert("RGB")
#         img_np = np.array(img)
#         st.image(img_np, caption="Uploaded Image", use_column_width=True)
#         out_img, labels = run_inference(img_np)
#         st.image(out_img, caption="Predicted", use_column_width=True)
#         st.write("Detected Signs:", labels)
# 
# # ----- Camera -----
# elif mode == "Use Camera":
#     img_file = st.camera_input("Take a picture")
#     if img_file:
#         img = Image.open(img_file).convert("RGB")
#         img_np = np.array(img)
#         out_img, labels = run_inference(img_np)
#         st.image(out_img, caption="Prediction", use_column_width=True)
#         st.write("Detected Signs:", labels)
# 
# # ----- Video -----
# else:
#     video_file = st.file_uploader("Upload a video", type=["mp4", "mov", "avi"])
#     if video_file:
#         tfile = tempfile.NamedTemporaryFile(delete=False)
#         tfile.write(video_file.read())
#         cap = cv2.VideoCapture(tfile.name)
#         stframe = st.empty()
#         sequence = []
#         window = deque(maxlen=7)
# 
#         while cap.isOpened():
#             ret, frame = cap.read()
#             if not ret:
#                 break
#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             out_img, labels = run_inference(frame_rgb)
# 
#             pred = labels[0][0] if labels else None
#             if pred:
#                 window.append(pred)
#                 most = Counter(window).most_common(1)[0][0]
#                 if not sequence or sequence[-1] != most:
#                     sequence.append(most)
# 
#             stframe.image(out_img, use_column_width=True)
#             st.markdown(f"### ðŸ§  Predicted Sequence: {' '.join(sequence)}")
# 
#         cap.release()
#         os.unlink(tfile.name)
#

!pip install -r /content/asl_streamlit/requirements.txt
!streamlit run /content/asl_streamlit/app.py --server.port 8081

!pip install pyngrok